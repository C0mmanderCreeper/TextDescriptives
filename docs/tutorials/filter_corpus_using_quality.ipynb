{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering corpora using Quality\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/HLasse/TextDescriptives/blob/main/docs/tutorials/filter_corpus_using_quality.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "In many cases if you want to analyse tweets, train a model on text scraped from the web or similar, it is important to filter out low-quality texts.\n",
    "\n",
    "TextDescriptives implements a series of heuristic filters for removing low-quality text. This tutorial will take you through how to use these to filter\n",
    "your text corpora."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "For this we will use the [Danish Gigaword](https://sprogteknologi.dk/dataset/danish-gigaword) available on [Huggingface Datasets](DDSC/partial-danish-gigaword-no-twitter). A large collection of Danish texts collected from a variety of domains. To download it we will need the `datasets` package. To install it please run:\n",
    "\n",
    "```python\n",
    "!pip install datasets\n",
    "```\n",
    "\n",
    "We can now easily donwload the dataset using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration DDSC--partial-danish-gigaword-no-twitter-d9c41a85c2339e48\n",
      "Found cached dataset parquet (/Users/au561649/.cache/huggingface/datasets/DDSC___parquet/DDSC--partial-danish-gigaword-no-twitter-d9c41a85c2339e48/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae17b122ba40474e83cb277625131e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# note this can take quite a while\n",
    "dataset = load_dataset(\"DDSC/partial-danish-gigaword-no-twitter\")\n",
    "\n",
    "# All of the dataset is available in the train split - we can simply:\n",
    "dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>LICENSE</th>\n",
       "      <th>uri</th>\n",
       "      <th>date_built</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JØRGINE JØRGINE KØBENHAVN HAGE &amp; CLAUSENS FORL...</td>\n",
       "      <td>jvj</td>\n",
       "      <td>jvj_Jørgine</td>\n",
       "      <td>Attribution-ShareAlike 4.0 International</td>\n",
       "      <td>NA</td>\n",
       "      <td>Fri Jun 26 13:06:11 2020 CEST +0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MYTER MYTER NY SAMLING GYLDENDALSKE BOGHANDEL...</td>\n",
       "      <td>jvj</td>\n",
       "      <td>jvj_Myter-ny-samling</td>\n",
       "      <td>Attribution-ShareAlike 4.0 International</td>\n",
       "      <td>NA</td>\n",
       "      <td>Fri Jun 26 13:06:01 2020 CEST +0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DEN NY VERDEN DEN NY VERDEN TIL INTERNATIONAL ...</td>\n",
       "      <td>jvj</td>\n",
       "      <td>jvj_Den-ny-Verden</td>\n",
       "      <td>Attribution-ShareAlike 4.0 International</td>\n",
       "      <td>NA</td>\n",
       "      <td>Fri Jun 26 13:06:45 2020 CEST +0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CIMBRERNES TOG TIL EMMERIK JENSEN F . 15 . MAJ...</td>\n",
       "      <td>jvj</td>\n",
       "      <td>jvj_Cimbrernes-Tog</td>\n",
       "      <td>Attribution-ShareAlike 4.0 International</td>\n",
       "      <td>NA</td>\n",
       "      <td>Fri Jun 26 13:05:56 2020 CEST +0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OM SPROGET OG UNDERVISNINGEN OM SPROGET OG UND...</td>\n",
       "      <td>jvj</td>\n",
       "      <td>jvj_Om-Sproget-og-Undervisningen</td>\n",
       "      <td>Attribution-ShareAlike 4.0 International</td>\n",
       "      <td>NA</td>\n",
       "      <td>Fri Jun 26 13:05:49 2020 CEST +0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GÆST KOMMER TIL VERDEN HAN var født paa Sjælan...</td>\n",
       "      <td>jvj</td>\n",
       "      <td>jvj_Gæst-kommer-til-Verden</td>\n",
       "      <td>Attribution-ShareAlike 4.0 International</td>\n",
       "      <td>NA</td>\n",
       "      <td>Fri Jun 26 13:06:21 2020 CEST +0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MYTER OG JAGTER MYTER OG JAGTER GYLDENDALSKE B...</td>\n",
       "      <td>jvj</td>\n",
       "      <td>jvj_Myter-og-Jagter</td>\n",
       "      <td>Attribution-ShareAlike 4.0 International</td>\n",
       "      <td>NA</td>\n",
       "      <td>Fri Jun 26 13:06:14 2020 CEST +0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DET TABTE LAND DET TABTE LAND, MENNESKET FØR I...</td>\n",
       "      <td>jvj</td>\n",
       "      <td>jvj_Det-tabte-Land</td>\n",
       "      <td>Attribution-ShareAlike 4.0 International</td>\n",
       "      <td>NA</td>\n",
       "      <td>Fri Jun 26 13:06:50 2020 CEST +0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SANGERINDEN SANGERINDEN (MADAME D'ORA) DRAMA I...</td>\n",
       "      <td>jvj</td>\n",
       "      <td>jvj_Sangerinden</td>\n",
       "      <td>Attribution-ShareAlike 4.0 International</td>\n",
       "      <td>NA</td>\n",
       "      <td>Fri Jun 26 13:06:52 2020 CEST +0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DYRENES FORVANDLING DYRENES FORVANDLING TIL UD...</td>\n",
       "      <td>jvj</td>\n",
       "      <td>jvj_Dyrenes-Forvandling</td>\n",
       "      <td>Attribution-ShareAlike 4.0 International</td>\n",
       "      <td>NA</td>\n",
       "      <td>Fri Jun 26 13:06:48 2020 CEST +0200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text source  \\\n",
       "0  JØRGINE JØRGINE KØBENHAVN HAGE & CLAUSENS FORL...    jvj   \n",
       "1   MYTER MYTER NY SAMLING GYLDENDALSKE BOGHANDEL...    jvj   \n",
       "2  DEN NY VERDEN DEN NY VERDEN TIL INTERNATIONAL ...    jvj   \n",
       "3  CIMBRERNES TOG TIL EMMERIK JENSEN F . 15 . MAJ...    jvj   \n",
       "4  OM SPROGET OG UNDERVISNINGEN OM SPROGET OG UND...    jvj   \n",
       "5  GÆST KOMMER TIL VERDEN HAN var født paa Sjælan...    jvj   \n",
       "6  MYTER OG JAGTER MYTER OG JAGTER GYLDENDALSKE B...    jvj   \n",
       "7  DET TABTE LAND DET TABTE LAND, MENNESKET FØR I...    jvj   \n",
       "8  SANGERINDEN SANGERINDEN (MADAME D'ORA) DRAMA I...    jvj   \n",
       "9  DYRENES FORVANDLING DYRENES FORVANDLING TIL UD...    jvj   \n",
       "\n",
       "                             doc_id                                   LICENSE  \\\n",
       "0                       jvj_Jørgine  Attribution-ShareAlike 4.0 International   \n",
       "1              jvj_Myter-ny-samling  Attribution-ShareAlike 4.0 International   \n",
       "2                 jvj_Den-ny-Verden  Attribution-ShareAlike 4.0 International   \n",
       "3                jvj_Cimbrernes-Tog  Attribution-ShareAlike 4.0 International   \n",
       "4  jvj_Om-Sproget-og-Undervisningen  Attribution-ShareAlike 4.0 International   \n",
       "5        jvj_Gæst-kommer-til-Verden  Attribution-ShareAlike 4.0 International   \n",
       "6               jvj_Myter-og-Jagter  Attribution-ShareAlike 4.0 International   \n",
       "7                jvj_Det-tabte-Land  Attribution-ShareAlike 4.0 International   \n",
       "8                   jvj_Sangerinden  Attribution-ShareAlike 4.0 International   \n",
       "9           jvj_Dyrenes-Forvandling  Attribution-ShareAlike 4.0 International   \n",
       "\n",
       "  uri                           date_built  \n",
       "0  NA  Fri Jun 26 13:06:11 2020 CEST +0200  \n",
       "1  NA  Fri Jun 26 13:06:01 2020 CEST +0200  \n",
       "2  NA  Fri Jun 26 13:06:45 2020 CEST +0200  \n",
       "3  NA  Fri Jun 26 13:05:56 2020 CEST +0200  \n",
       "4  NA  Fri Jun 26 13:05:49 2020 CEST +0200  \n",
       "5  NA  Fri Jun 26 13:06:21 2020 CEST +0200  \n",
       "6  NA  Fri Jun 26 13:06:14 2020 CEST +0200  \n",
       "7  NA  Fri Jun 26 13:06:50 2020 CEST +0200  \n",
       "8  NA  Fri Jun 26 13:06:52 2020 CEST +0200  \n",
       "9  NA  Fri Jun 26 13:06:48 2020 CEST +0200  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can take a look at one of the examples:\n",
    "ten_samples = dataset.select(range(10))\n",
    "ten_samples.to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned the Danish Gigaword consist of multiple domains. For this tutorial, we will look at three of these domains. `retsinformationdk` which consist of legal documents, `wiki` which contain Wikipedia articles and `spont` which contains texts transcriped from spontaneous speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/au561649/.cache/huggingface/datasets/DDSC___parquet/DDSC--partial-danish-gigaword-no-twitter-d9c41a85c2339e48/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-6e6efda35614635a.arrow\n",
      "Loading cached processed dataset at /Users/au561649/.cache/huggingface/datasets/DDSC___parquet/DDSC--partial-danish-gigaword-no-twitter-d9c41a85c2339e48/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-3ce9447c21439e3f.arrow\n",
      "Loading cached processed dataset at /Users/au561649/.cache/huggingface/datasets/DDSC___parquet/DDSC--partial-danish-gigaword-no-twitter-d9c41a85c2339e48/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-6528b379c635e45c.arrow\n"
     ]
    }
   ],
   "source": [
    "# we can filter out these three datasets based on the \"source\"\n",
    "legal = dataset.filter(lambda x: x[\"source\"] == \"retsinformationdk\")\n",
    "wiki = dataset.filter(lambda x: x[\"source\"] == \"wiki\")\n",
    "speech = dataset.filter(lambda x: x[\"source\"] == \"spont\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now examine these datasets a bit more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legal contains 64043 examples\n",
      "Wiki contains 425938 examples\n",
      "Speech contains 411 examples\n"
     ]
    }
   ],
   "source": [
    "print(f\"Legal contains {len(legal)} examples\")\n",
    "print(f\"Wiki contains {len(wiki)} examples\")\n",
    "print(f\"Speech contains {len(speech)} examples\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can for example see that the speech dataset contains notably fewer sampels than the rest. So let us downsample the rest to ~1000 samples each before we start the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legal now contains 1000 examples\n",
      "Wiki now contains 1000 examples\n"
     ]
    }
   ],
   "source": [
    "legal = legal.select(range(1000))\n",
    "wiki = wiki.select(range(1000))\n",
    "\n",
    "print(f\"Legal now contains {len(legal)} examples\")\n",
    "print(f\"Wiki now contains {len(wiki)} examples\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Filtering\n",
    "After we have prepared our datasets we can now start with the quality filtering. Using Textdescriptives this is extremely simple. We need to do 3 thing:\n",
    "\n",
    "1) Create a pipeline\n",
    "2) Add the quality component from textdescriptives to it\n",
    "3) Apply the pipeline to the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textdescriptives as td\n",
    "\n",
    "# 1. Crease a blank spaCy model with a sentencizer\n",
    "nlp = spacy.blank(\"da\")\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "nlp.max_length = 2000000  # as some of the documents are quite long we can increase the max length\n",
    "# however it might be worth filtering out these documents before for very very long documents.\n",
    "\n",
    "# 2. Add the textdescriptives pipeline\n",
    "quality_pipe = nlp.add_pipe(\"textdescriptives/quality\")\n",
    "\n",
    "# 3. Apply the pipeline to the legal documents\n",
    "legal_docs = nlp.pipe(legal[\"text\"], batch_size=100, n_process=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check now we can see that legal_docs is a generator. This can be a quite efficient format, but for now we just want to process all the text so we simply need to convert it to a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Language.pipe at 0x415ab9c10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legal_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_docs = list(legal_docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the output here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Den fulde tekst Pressenævnets kendelse i sag nr. 15-70-00822\n",
      "Resumé\n",
      "Foreningen for Skånsomt Kystfiskeri har ikke retlig interesse\n",
      "DR bragte et radioindslag om Natur- og Erhvervsstyrelsens fiskeriinspektorats fangst af ulovlige ålefælder. Foreningen for Skånsomt Kystfiskeri klagede blandt andet med den begrundelse, at betegnelsen ” ålefælder ” er forkert, idet ålene selv kan svømme ind og ud. Pressenævnet afviser at behandle klagen, da foreningen ikke er omtalt i udsendelsen og derfor ikke har retlig interesse.\n",
      "Pressenævnets formand udtaler:\n",
      "Det er en betingelse for at klage til Pressenævnet, at\n",
      "----\n",
      "This is pass the quality filter:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legal_doc = legal_docs[0]\n",
    "\n",
    "print(legal_doc[:100]) # print the first 100 tokens\n",
    "print(\"----\")\n",
    "print(\"This is pass the quality filter:\")\n",
    "legal_doc._.passed_quality_check"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the text did not pass the quality filter. We can now examine why that using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_stop_words': 192,\n",
       " 'alpha_ratio': 0.804,\n",
       " 'mean_word_length': 4.546,\n",
       " 'doc_length': 500,\n",
       " 'proportion_ellipsis': 0.0,\n",
       " 'proportion_bullet_points': 0.0,\n",
       " 'duplicate_line_chr_fraction': 0.25737766156144937,\n",
       " 'duplicate_paragraph_chr_fraction': 0.0,\n",
       " 'duplicate_5-gram_chr_fraction': 0.5401568920433321,\n",
       " 'duplicate_6-gram_chr_fraction': 0.519237952932387,\n",
       " 'duplicate_7-gram_chr_fraction': 0.519237952932387,\n",
       " 'duplicate_8-gram_chr_fraction': 0.519237952932387,\n",
       " 'duplicate_9-gram_chr_fraction': 0.519237952932387,\n",
       " 'duplicate_10-gram_chr_fraction': 0.519237952932387,\n",
       " 'top_2-gram_chr_fraction': 0.017930519237952934,\n",
       " 'top_3-gram_chr_fraction': 0.042958535674262235,\n",
       " 'top_4-gram_chr_fraction': 0.0653716847217034,\n",
       " 'symbol_#_to_word_ratio': 0.0,\n",
       " 'contains_lorem ipsum': False}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legal_doc._.quality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that fraction of characters which is a part of a duplicate 10 gram is >50%. This is likely the reason why the sample was filtered out. This is not uncommon for legal documents which contain a lot of standard phrases. However you might wish to change the threshold for this filter. You can see an example of how to do this in the [documentation](file:///Users/au561649/Github/TextDescriptives/docs/_build/html/quality.html).\n",
    "\n",
    "You can also inspect the existing thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QualityThresholds(n_stop_words=(2, None), alpha_ratio=(0.8, None), mean_word_length=(3, 10), doc_length=(10, 100000), symbol_to_word_ratio={'#': (None, 0.1)}, proportion_ellipsis=(None, 0.3), proportion_bullet_points=(None, 0.8), contains={'lorem ipsum': False}, duplicate_line_chr_fraction=(None, 0.2), duplicate_paragraph_chr_fraction=(None, 0.2), duplicate_ngram_chr_fraction={'5': (None, 0.15), '6': (None, 0.14), '7': (None, 0.13), '8': (None, 0.12), '9': (None, 0.11), '10': (None, 0.1)}, top_ngram_chr_fraction={'2': (None, 0.2), '3': (None, 0.18), '4': (None, 0.16)})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quality_pipe.quality_thresholds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the `duplicate_ngram_chr_fraction` for 10-grams is 0.1. This means that if a text contains more than 10% of characters which are a part of a duplicate 10-gram it will be filtered out."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out the text\n",
    "Assuming we don't want to change the filter we can now use it to filter out the texts that we want to keep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Filter out the documents that do not pass the quality\n",
    "legal_docs_filtered = [doc for doc in legal_docs if doc._.passed_quality_check]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We had a total of 1000 which we filtered down to 68.\n"
     ]
    }
   ],
   "source": [
    "print(f\"We had a total of {len(legal['text'])} which we filtered down to {len(legal_docs_filtered)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textdescriptives",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31387647799921bb85032eec7bb02e281325ae7f8ffa6f9cd7cdead815b36c88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
